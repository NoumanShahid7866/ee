{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1dfdF2k69t1"
      },
      "source": [
        "# **Connecting to Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "a9sTKZDofYZf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd9ea5f8-0831-404b-c986-2ab4c2f36be3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxk6Sn7b7Hxq"
      },
      "source": [
        "# **Mapping Image-IDs to Labels**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LK_7jI6pfZ_p"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the labels.csv file\n",
        "labels_df = pd.read_csv('/content/drive/MyDrive/labels.csv')\n",
        "\n",
        "# Create a dictionary that maps image IDs to their corresponding labels\n",
        "labels_dict = {}\n",
        "for index, row in labels_df.iterrows():\n",
        "    labels_dict[row['image_id']] = row['Object']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1OlxDJC7U0Q"
      },
      "source": [
        "# **Split data by train_test_split**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "T2jg6SBj7pSg"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GaxvSzH6f9nO"
      },
      "outputs": [],
      "source": [
        "train_image_ids, test_image_ids, train_labels, test_labels = train_test_split(list(labels_dict.keys()),list(labels_dict.values()), test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjhjRo4o7hpW"
      },
      "source": [
        "# **Load the Images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KSu_Yhx676HD"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WT2qTlHVgAlK"
      },
      "outputs": [],
      "source": [
        "image_size = (224, 224)\n",
        "train_images = []\n",
        "test_images = []\n",
        "for image_id in train_image_ids:\n",
        "    image_path = f'/content/drive/MyDrive/Archaeological_Objects_Cropped/{image_id}.jpg'\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.resize(image, image_size)\n",
        "    train_images.append(image)\n",
        "\n",
        "for image_id in test_image_ids:\n",
        "    image_path = f'/content/drive/MyDrive/Archaeological_Objects_Cropped/{image_id}.jpg'\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.resize(image, image_size)\n",
        "    test_images.append(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4cQdUUe8JI5"
      },
      "source": [
        "# **Convert the lists to NumPy arrays**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WevCgnsC8F1x"
      },
      "outputs": [],
      "source": [
        "train_images = np.array(train_images)\n",
        "test_images = np.array(test_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gx_s-NG88RH_"
      },
      "source": [
        "# **Defining a backbone Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5o121sa92W5"
      },
      "source": [
        "*A backbone model is typically a convolutional neural network (CNN) that has been pre-trained on a large dataset, such as ImageNet, to learn general features that can be applied to a wide range of tasks. The pre-trained model is then fine-tuned on a smaller dataset specific to the task at hand, such as object detection, segmentation, or classification.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YQfSR8dV8YW7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0a150e7-7fe9-473b-e667-bc47df279f98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "from keras.applications import VGG16\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from keras.regularizers import l2\n",
        "from keras.models import Model\n",
        "backbone = VGG16(weights='imagenet', include_top=False, input_shape=(image_size[0], image_size[1], 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eL2EZhSU8tUX"
      },
      "source": [
        "# **Define the RPN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIlpPV66-FC2"
      },
      "source": [
        "*RPN is a neural network that generates region proposals, which are regions of interest in an image that may contain objects. The RPN takes an image as input and outputs a set of bounding boxes, called anchors, that are likely to contain objects. These anchors are then used as input to a classifier and regressor to refine the object detection process.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "w2o7EFDI8owC"
      },
      "outputs": [],
      "source": [
        "rpn = Conv2D(512, (3, 3), activation='relu', padding='same',  kernel_regularizer=l2(0.01))(backbone.output)\n",
        "rpn = Conv2D(512, (3, 3), activation='relu', padding='same',  kernel_regularizer=l2(0.01))(rpn)\n",
        "rpn = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.01))(rpn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dn0MSfyi87pq"
      },
      "source": [
        "# **Define the classification layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6UPB96VLgZkR"
      },
      "outputs": [],
      "source": [
        "classification = Flatten()(rpn)\n",
        "classification = Dense(4096, activation='relu', kernel_regularizer=l2(0.01))(classification)\n",
        "classification = Dropout(0.5)(classification)\n",
        "classification = Dense(len(labels_dict), activation='softmax')(classification)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HP5k4s569B77"
      },
      "source": [
        "# **Define the faster R-CNN model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnTf5Tf1-WnG"
      },
      "source": [
        "*Faster R-CNN is a popular object detection algorithm that builds upon the success of R-CNN and its variants. It's a powerful tool for detecting objects in images and videos, and has been widely adopted in various applications, including self-driving cars, surveillance systems, and medical image analysis.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cAgFuYWG9Ix3"
      },
      "outputs": [],
      "source": [
        "model = Model(inputs=backbone.input, outputs=[rpn, classification])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4w2OFwP9L5n"
      },
      "source": [
        "# **Compile the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7MZruEARh2vu"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=['mean_squared_error', 'categorical_crossentropy'], optimizer='adam', metrics=[['accuracy'] , ['accuracy']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOZ415CV9WY9"
      },
      "source": [
        "# **Train the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "tDXPD9n6GFKv"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dshiCOP0qlr7",
        "outputId": "06b7a8f7-bb1a-462e-d91f-a5283f31a7f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1120s\u001b[0m 80s/step - conv2d_2_accuracy: 0.5076 - loss: 71.9931 - val_conv2d_2_accuracy: 1.0000 - val_loss: 14.0735\n",
            "Epoch 2/10\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1177s\u001b[0m 82s/step - conv2d_2_accuracy: 0.9720 - loss: 13.7828 - val_conv2d_2_accuracy: 0.8822 - val_loss: 10.5664\n",
            "Epoch 3/10\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1168s\u001b[0m 82s/step - conv2d_2_accuracy: 0.9318 - loss: 9.3447 - val_conv2d_2_accuracy: 0.8967 - val_loss: 7.0076\n",
            "Epoch 4/10\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1138s\u001b[0m 82s/step - conv2d_2_accuracy: 0.9503 - loss: 6.5640 - val_conv2d_2_accuracy: 0.9592 - val_loss: 5.2817\n",
            "Epoch 5/10\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1134s\u001b[0m 81s/step - conv2d_2_accuracy: 0.9615 - loss: 4.9698 - val_conv2d_2_accuracy: 0.8633 - val_loss: 4.1521\n",
            "Epoch 6/10\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1149s\u001b[0m 81s/step - conv2d_2_accuracy: 0.9148 - loss: 3.9334 - val_conv2d_2_accuracy: 0.9471 - val_loss: 3.3418\n",
            "Epoch 7/10\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1187s\u001b[0m 82s/step - conv2d_2_accuracy: 0.8846 - loss: 3.1833 - val_conv2d_2_accuracy: 0.9500 - val_loss: 2.7455\n",
            "Epoch 8/10\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1162s\u001b[0m 82s/step - conv2d_2_accuracy: 0.8933 - loss: 2.6252 - val_conv2d_2_accuracy: 0.9957 - val_loss: 2.2909\n",
            "Epoch 9/10\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1153s\u001b[0m 83s/step - conv2d_2_accuracy: 0.9059 - loss: 2.1979 - val_conv2d_2_accuracy: 0.8342 - val_loss: 1.9379\n",
            "Epoch 10/10\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1165s\u001b[0m 83s/step - conv2d_2_accuracy: 0.8624 - loss: 1.8647 - val_conv2d_2_accuracy: 0.9229 - val_loss: 1.6594\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7c649d460310>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "model.fit(train_images,\n",
        "          [np.zeros((len(train_images), 7, 7, 512))],\n",
        "          epochs=10,\n",
        "          batch_size=32,\n",
        "          validation_data=(test_images, [np.zeros((len(test_images), 7, 7, 512))]),\n",
        "          callbacks=[EarlyStopping(patience=5)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yxuRrL59b65"
      },
      "source": [
        "#**Save the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "LKJB4L0hiVgV"
      },
      "outputs": [],
      "source": [
        "model.save('archeological_artifacts_model.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QdmpiI8Gl9i"
      },
      "source": [
        "#**Evaluate the model on the test data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9qXkCnMGuHp"
      },
      "outputs": [],
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_images, [np.zeros((len(test_images), 7, 7, 512))], verbose=0)\n",
        "print(f'Test loss: {test_loss:.3f}')\n",
        "print(f'Test accuracy: {test_accuracy:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Mean Average Precision (mAP)**\n",
        "It's a metric used to evaluate the performance of object detection models, particularly in computer vision tasks.\n",
        "\n",
        "In object detection, the goal is to locate objects within an image and classify them into different categories (e.g., person, car, dog, etc.). The model predicts bounding boxes around the objects, along with a confidence score indicating how likely the object is to be present."
      ],
      "metadata": {
        "id": "v29lv8jSOPah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Calculation of mAP**"
      ],
      "metadata": {
        "id": "ArBASbdQPCgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "def calculate_ap(y_true, y_pred):\n",
        "    if np.sum(y_true) == 0:\n",
        "        return 0\n",
        "    ap = average_precision_score(y_true, y_pred)\n",
        "    return ap\n",
        "\n",
        "def calculate_map(y_true, y_pred, labels_dict):\n",
        "    aps = []\n",
        "    for i in range(len(labels_dict)):\n",
        "        y_true_class = np.array([1 if label == i else 0 for label in y_true])\n",
        "        y_pred_class = y_pred[:, i]\n",
        "        ap = calculate_ap(y_true_class, y_pred_class)\n",
        "        aps.append(ap)\n",
        "    map = np.mean(aps)\n",
        "    return map\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(test_images)[1]\n",
        "\n",
        "# Convert the predictions to a binary format (0 or 1) for each class\n",
        "y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# Define labels_dict\n",
        "labels_dict = {i: f'class{i}' for i in range(y_pred.shape[1])}\n",
        "\n",
        "# Calculate the mean average precision (mAP)\n",
        "map = calculate_map(test_labels, y_pred_binary, labels_dict)\n",
        "print(f'mAP: {map:.3f}')"
      ],
      "metadata": {
        "id": "qPUdHHrLNkR_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}